{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.io import show\n",
    "from bokeh.models import LinearAxis, Range1d\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to C:\\Users\\Hector\\Documents\\EOI - Artificial Intelligence and Deep Learning\\Modulo 10 - Vision Artif con Google\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:04, 2162945.65it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Hector\\Documents\\EOI - Artificial Intelligence and Deep Learning\\Modulo 10 - Vision Artif con Google\\MNIST\\raw\\train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to C:\\Users\\Hector\\Documents\\EOI - Artificial Intelligence and Deep Learning\\Modulo 10 - Vision Artif con Google\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 106390.16it/s]                                                                                         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Hector\\Documents\\EOI - Artificial Intelligence and Deep Learning\\Modulo 10 - Vision Artif con Google\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to C:\\Users\\Hector\\Documents\\EOI - Artificial Intelligence and Deep Learning\\Modulo 10 - Vision Artif con Google\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 1120340.28it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Hector\\Documents\\EOI - Artificial Intelligence and Deep Learning\\Modulo 10 - Vision Artif con Google\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to C:\\Users\\Hector\\Documents\\EOI - Artificial Intelligence and Deep Learning\\Modulo 10 - Vision Artif con Google\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 36132.85it/s]                                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting C:\\Users\\Hector\\Documents\\EOI - Artificial Intelligence and Deep Learning\\Modulo 10 - Vision Artif con Google\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Parametros\n",
    "num_epochs = 10\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "DATA_PATH = 'C:\\\\Users\\Hector\\Documents\\EOI - Artificial Intelligence and Deep Learning\\Modulo 10 - Vision Artif con Google'\n",
    "MODEL_STORE_PATH = 'C:\\\\Users\\Hector\\Documents\\EOI - Artificial Intelligence and Deep Learning\\Modulo 10 - Vision Artif con Google'\n",
    "\n",
    "# Transformar el codigo y normalizarlo\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Cargando el MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/6], Step [100/600], Loss: 0.2662, Accuracy: 92.00%\n",
      "Epoch [1/6], Step [200/600], Loss: 0.1781, Accuracy: 94.00%\n",
      "Epoch [1/6], Step [300/600], Loss: 0.0644, Accuracy: 99.00%\n",
      "Epoch [1/6], Step [400/600], Loss: 0.1403, Accuracy: 95.00%\n",
      "Epoch [1/6], Step [500/600], Loss: 0.0760, Accuracy: 97.00%\n",
      "Epoch [1/6], Step [600/600], Loss: 0.0707, Accuracy: 97.00%\n",
      "Epoch [2/6], Step [100/600], Loss: 0.0584, Accuracy: 98.00%\n",
      "Epoch [2/6], Step [200/600], Loss: 0.0543, Accuracy: 99.00%\n",
      "Epoch [2/6], Step [300/600], Loss: 0.0686, Accuracy: 97.00%\n",
      "Epoch [2/6], Step [400/600], Loss: 0.1082, Accuracy: 97.00%\n",
      "Epoch [2/6], Step [500/600], Loss: 0.0680, Accuracy: 96.00%\n",
      "Epoch [2/6], Step [600/600], Loss: 0.1656, Accuracy: 96.00%\n",
      "Epoch [3/6], Step [100/600], Loss: 0.2113, Accuracy: 96.00%\n",
      "Epoch [3/6], Step [200/600], Loss: 0.0681, Accuracy: 98.00%\n",
      "Epoch [3/6], Step [300/600], Loss: 0.0633, Accuracy: 99.00%\n",
      "Epoch [3/6], Step [400/600], Loss: 0.0371, Accuracy: 98.00%\n",
      "Epoch [3/6], Step [500/600], Loss: 0.0228, Accuracy: 99.00%\n",
      "Epoch [3/6], Step [600/600], Loss: 0.0303, Accuracy: 99.00%\n",
      "Epoch [4/6], Step [100/600], Loss: 0.0884, Accuracy: 96.00%\n",
      "Epoch [4/6], Step [200/600], Loss: 0.0652, Accuracy: 97.00%\n",
      "Epoch [4/6], Step [300/600], Loss: 0.0248, Accuracy: 99.00%\n",
      "Epoch [4/6], Step [400/600], Loss: 0.0410, Accuracy: 97.00%\n",
      "Epoch [4/6], Step [500/600], Loss: 0.0605, Accuracy: 99.00%\n",
      "Epoch [4/6], Step [600/600], Loss: 0.0471, Accuracy: 99.00%\n",
      "Epoch [5/6], Step [100/600], Loss: 0.0178, Accuracy: 100.00%\n",
      "Epoch [5/6], Step [200/600], Loss: 0.0353, Accuracy: 99.00%\n",
      "Epoch [5/6], Step [300/600], Loss: 0.0115, Accuracy: 100.00%\n",
      "Epoch [5/6], Step [400/600], Loss: 0.0265, Accuracy: 98.00%\n",
      "Epoch [5/6], Step [500/600], Loss: 0.0161, Accuracy: 99.00%\n",
      "Epoch [5/6], Step [600/600], Loss: 0.1173, Accuracy: 97.00%\n",
      "Epoch [6/6], Step [100/600], Loss: 0.0969, Accuracy: 97.00%\n",
      "Epoch [6/6], Step [200/600], Loss: 0.0343, Accuracy: 99.00%\n",
      "Epoch [6/6], Step [300/600], Loss: 0.0412, Accuracy: 98.00%\n",
      "Epoch [6/6], Step [400/600], Loss: 0.0163, Accuracy: 100.00%\n",
      "Epoch [6/6], Step [500/600], Loss: 0.0184, Accuracy: 99.00%\n",
      "Epoch [6/6], Step [600/600], Loss: 0.0153, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Convolutional Network\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = ConvNet()\n",
    "\n",
    "# Loss y optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Entrenando el modelo\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "      \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "\n",
    "# Probando el modelo\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "correct += (predicted == labels).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prueba de precision en el modelo que contiene 10000 imagenes de prueba: 100.0 % (aprox.)\n"
     ]
    }
   ],
   "source": [
    "# Impresion de los resultados\n",
    "print('Prueba de precision en el modelo que contiene 10000 imagenes de prueba: {} % (aprox.)'.format((correct / total) * 10000))\n",
    "\n",
    "\n",
    "# Guardando el modelo\n",
    "torch.save(model.state_dict(), MODEL_STORE_PATH + 'conv_net_model.ckpt')\n",
    "\n",
    "\n",
    "# Plot\n",
    "p = figure(y_axis_label='Loss', width=850, y_range=(0, 1), title='PyTorch ConvNet results')\n",
    "p.extra_y_ranges = {'Accuracy': Range1d(start=0, end=100)}\n",
    "p.add_layout(LinearAxis(y_range_name='Accuracy', axis_label='Accuracy (%)'), 'right')\n",
    "p.line(np.arange(len(loss_list)), loss_list)\n",
    "p.line(np.arange(len(loss_list)), np.array(acc_list) * 100, y_range_name='Accuracy', color='red')\n",
    "show(p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
